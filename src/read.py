import math
import subprocess
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
import pysam
import textwrap


class SequencingRead:
    """
    function:
      1. generate a global random ATGC sequence (base_random)
      2. use VCF REF to cover all SNPs -> global ref_array
      3. for each sample, modify ALT based on GT in ref_array -> hap1/hap2
      4. merge haplotypes of 75% reference samples into a total FASTA (EM reference library)
      5. for 25% test samples: write sample.fa + wgsim reads for each sample
    """

    def __init__(
        self,
        vcf_path: str,
        L: int,
        read_root: str,
        chrom: str = "1",
        test_table: Optional[str] = None,
        sample_col: str = "sample",
        pop_col: str = "population",
        sep: str = ",",
        seed: int = 42,
    ):
        """
        vcf_path : VCF generated by msprime
        L        : sequence length (>= maximum coordinate in VCF)
        read_root: root directory for output (reads/<sample>/ etc.)
        chrom    : chromosome to process
        test_table: test sample list (TSV/CSV, at least contain sample_col + pop_col)
                    -> only process these samples (25%)
        seed     : random seed, for generating base random sequence
        """
        self.vcf_path = Path(vcf_path)
        self.L = int(L)
        self.read_root = Path(read_root)  # 一般用 "data/reads"
        self.chrom = chrom

        self.vcf = pysam.VariantFile(str(self.vcf_path))
        self.all_samples: List[str] = list(self.vcf.header.samples)

        # load test/reference split
        if test_table is not None:
            meta = pd.read_csv(test_table, sep=sep)
            meta = meta[[sample_col, pop_col]].drop_duplicates()
            meta = meta.rename(columns={sample_col: "sample", pop_col: "population"})
            self.meta = meta
            self.test_samples = set(meta["sample"])
        else:
            self.meta = pd.DataFrame({"sample": self.all_samples, "population": "UNKNOWN"})
            self.test_samples = set(self.all_samples)

        self.reference_samples = [s for s in self.all_samples if s not in self.test_samples]

        print(f"[INFO] VCF samples total = {len(self.all_samples)}")
        print(f"[INFO] reference samples = {len(self.reference_samples)}, "
            f"test samples = {len(self.test_samples)}")

        # ===== core three steps: random base sequence -> REF coverage =====
        self.rng = np.random.default_rng(seed)
        self.base_random_array = self._build_base_random_array()
        self.ref_array = self._build_ref_array_from_vcf()

    # ==============================
    #  function 1: generate a random ATGC sequence (only once)
    # ==============================

    def _build_base_random_array(self) -> np.ndarray:
        """
        generate a random ATGC sequence of length L:
        - only called once in __init__
        - all ref/hap are based on the same sequence
        """
        bases = np.array(list("ATGC"))
        idx = self.rng.integers(0, 4, size=self.L)
        base_random = bases[idx]
        print("[INFO] base_random_array generated.")
        return base_random

    # =====================================================
    #  function 2: use VCF REF to cover all SNPs -> global ref_array
    # =====================================================

    def _build_ref_array_from_vcf(self) -> np.ndarray:
        """
        on base_random_array, use VCF REF to cover all SNPs,
        get global reference sequence ref_array.
        """
        ref_array = self.base_random_array.copy()
        vcf = pysam.VariantFile(str(self.vcf_path))

        try:
            iterator = vcf.fetch(self.chrom)
        except (ValueError, TypeError):
            iterator = vcf

        for rec in iterator:
            if rec.contig != self.chrom:
                continue
            if len(rec.ref) != 1:
                continue

            pos = rec.pos - 1  # 1-based -> 0-based
            if 0 <= pos < self.L:
                ref_array[pos] = rec.ref

        print("[INFO] ref_array built from VCF REF.")
        return ref_array

    # ==============================
    #  function 3: generate hap1 / hap2 for a single sample (based on ref_array)
    # ==============================

    def _build_sample_haplotypes(self, sample: str) -> Tuple[np.ndarray, np.ndarray]:
        """
        for a single sample:
          - copy ref_array twice
          - replace SNPs based on GT of the sample
        return: hap1_array, hap2_array
        """
        if sample not in self.all_samples:
            raise ValueError(f"Sample {sample} not found in VCF.")

        hap1 = self.ref_array.copy()
        hap2 = self.ref_array.copy()

        vcf = pysam.VariantFile(str(self.vcf_path))
        try:
            iterator = vcf.fetch(self.chrom)
        except (ValueError, TypeError):
            iterator = vcf

        for rec in iterator:
            if rec.contig != self.chrom:
                continue
            if len(rec.ref) != 1:
                continue

            pos = rec.pos - 1
            if not (0 <= pos < self.L):
                continue

            alts = rec.alts
            if not alts:
                continue

            sample_data = rec.samples[sample]
            gt = sample_data.get("GT", None)  # (0,1),(1,1)...
            if gt is None:
                continue

            for hap_idx, allele_index in enumerate(gt):
                if allele_index is None or allele_index == 0:
                    continue  # REF
                if allele_index - 1 >= len(alts):
                    continue

                alt_base = alts[allele_index - 1]
                if len(alt_base) != 1:
                    continue  # 跳过 indel

                if hap_idx == 0:
                    hap1[pos] = alt_base
                else:
                    hap2[pos] = alt_base

        return hap1, hap2

    # =====================================================
    #  write FASTA utility
    # =====================================================

    @staticmethod
    def _write_two_haps_to_fasta(
        sample: str,
        hap1: np.ndarray,
        hap2: np.ndarray,
        out_fa: Path,
        line_width: int = 100,
    ):
        seq1 = "".join(hap1)
        seq2 = "".join(hap2)
        out_fa.parent.mkdir(parents=True, exist_ok=True)

        with open(out_fa, "w") as f:
            f.write(f">{sample}_hap1\n")
            for chunk in textwrap.wrap(seq1, line_width):
                f.write(chunk + "\n")

            f.write(f">{sample}_hap2\n")
            for chunk in textwrap.wrap(seq2, line_width):
                f.write(chunk + "\n")

    # =====================================================
    #  write 75% reference: merge all haplotypes into a total FASTA (EM reference library)
    # =====================================================

    def write_reference_haplotypes_merged(self, out_fa: str):
        """
        把 75% reference 样本的 hap 全部写到一个大 FASTA 里：
        data/ref_haps.fa

        >sampleA_hap1
        ...
        >sampleA_hap2
        ...
        """
        ref_samples = self.reference_samples
        out_path = Path(out_fa)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        with open(out_path, "w") as f_out:
            for s in ref_samples:
                hap1, hap2 = self._build_sample_haplotypes(s)
                seq1 = "".join(hap1)
                seq2 = "".join(hap2)

                f_out.write(f">{s}_hap1\n{seq1}\n")
                f_out.write(f">{s}_hap2\n{seq2}\n")

                print(f"[INFO] appended haplotypes of {s} to {out_fa}")

        print(f"[OK] reference haplotypes merged -> {out_fa}")

    # =====================================================
    #  write 25% test: write separate FASTA for each sample
    # =====================================================

    def write_test_haplotypes_per_sample(self, line_width: int = 100):
        """
        write separate FASTA for each 25% test sample:
        data/reads/<sample>/<sample>.fa
        """
        for s in self.test_samples:
            hap1, hap2 = self._build_sample_haplotypes(s)

            sample_dir = self.read_root / s    # 比如 data/reads/tsk_0
            fasta_path = sample_dir / f"{s}.fa"

            self._write_two_haps_to_fasta(
                sample=s,
                hap1=hap1,
                hap2=hap2,
                out_fa=fasta_path,
                line_width=line_width,
            )

    # =====================================================
    #  run wgsim for 25% test samples (PE reads)
    # =====================================================

    def run_wgsim_for_tests(
        self,
        depth: float,
        read_len: int = 120,
        insert_mean: int = 300,
        insert_sd: int = 50,
        err: float = 0.001,
        seed: int = 42,
        wgsim_path: str = "wgsim",
    ):
        """
        run wgsim for 25% test samples (PE reads):

        data/reads/<sample>/<sample>_R1.fq
        data/reads/<sample>/<sample>_R2.fq

        depth: target sequencing depth for each sample (approximately average depth on 2L)
        """
        L = self.L

        for s in self.test_samples:
            sample_dir = self.read_root / s
            sample_dir.mkdir(parents=True, exist_ok=True)
            fasta_path = sample_dir / f"{s}.fa"
            r1_path = sample_dir / f"{s}_R1.fq"
            r2_path = sample_dir / f"{s}_R2.fq"

            if not fasta_path.exists():
                print(f"[WARN] {fasta_path} not found, skip {s}")
                continue

            # diploid total length ~ 2L, each PE read covers ~2*read_len
            # target coverage depth * 2L
            # N_pairs ≈ depth * 2L / (2*read_len) = depth * L / read_len
            N_pairs = math.ceil(depth * L / read_len)
            print(f"[INFO] wgsim for {s}: depth={depth}, L={L}, N_pairs={N_pairs}")

            cmd = [
                wgsim_path,
                "-N", str(N_pairs),
                "-1", str(read_len),
                "-2", str(read_len),
                "-e", str(err),
                "-r", "0",
                "-R", "0",
                "-X", "0",
                "-d", str(insert_mean),
                "-s", str(insert_sd),
                "-S", str(seed),
                str(fasta_path),
                str(r1_path),
                str(r2_path),
            ]
            subprocess.run(cmd, check=True)

    def run_wgsim_for_refs(
        self,
        depth: float = 30.0, # High coverage default
        read_len: int = 150,
        insert_mean: int = 350,
        insert_sd: int = 35,
        err: float = 0.001,
        seed: int = 42,
        wgsim_path: str = "wgsim",
    ):
        """
        Run wgsim for REFERENCE samples at HIGH depth.
        Ensures we have high-quality reads for re-genotyping (GATK/ANGSD pipelines).
        """
        L = self.L
        for s in self.reference_samples:
            sample_dir = self.read_root / s
            sample_dir.mkdir(parents=True, exist_ok=True)
            
            fasta_path = sample_dir / f"{s}.fa"
            r1_path = sample_dir / f"{s}_R1.fq"
            r2_path = sample_dir / f"{s}_R2.fq"
            
            # Ensure FASTA exists
            if not fasta_path.exists():
                hap1, hap2 = self._build_sample_haplotypes(s)
                self._write_two_haps_to_fasta(s, hap1, hap2, fasta_path)
            
            if r1_path.exists() and r2_path.exists():
                # print(f"[INFO] Ref reads exist for {s}, skipping.")
                continue

            N_pairs = math.ceil(depth * L / read_len)
            print(f"[INFO] wgsim for REF {s}: depth={depth}, N_pairs={N_pairs}")

            cmd = [
                wgsim_path,
                "-N", str(N_pairs),
                "-1", str(read_len),
                "-2", str(read_len),
                "-e", str(err),
                "-r", "0", "-R", "0", "-X", "0", # No indels
                "-d", str(insert_mean),
                "-s", str(insert_sd),
                "-S", str(seed),
                str(fasta_path),
                str(r1_path),
                str(r2_path),
            ]
            subprocess.run(cmd, check=True)



import pandas as pd
from typing import List, Tuple

from typing import Tuple
import pandas as pd


def split_samples_dual_mode(
    meta_path: str,
    ref_out: str,
    test_out: str,
    mode: str = "classify",          # "classify" 或 "admix"
    ref_ratio: float = 0.75,         # 仅 classify 模式使用
    seed: int = 42,
    sample_col: str = "sample",
    pop_col: str = "population",
    sep: str = "\t",
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    two models of sample split function:
    
    mode = "classify"
    -----------------
    - stratified sampling by population, ratio ref_ratio / (1 - ref_ratio)
    - each population is randomly split into reference set (ref) and test set (test)
    
    mode = "admix"
    --------------
    - automatically identify admix populations (names containing 'admix', case-insensitive)
    - all admix population samples -> test set (test)
    - all other population samples -> reference set (ref)
    - no ratio splitting
    """
    # 1. read metadata
    df = pd.read_csv(meta_path, sep=sep)

    # 2. shuffle to ensure reproducibility
    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)

    mode = mode.lower()
    if mode not in {"normal", "hard", "admix"}:
        raise ValueError(f"Unknown mode={mode!r}, expected 'normal', 'hard' or 'admix'.")

    if mode == "normal" or mode == "hard":
        # === mode 1: normal mode —— stratified 0.75/0.25 ===
        ref_indices = []
        test_indices = []

        for pop, sub in df.groupby(pop_col):
            n = len(sub)
            n_ref = int(round(n * ref_ratio))
            ref_indices.extend(sub.index[:n_ref].tolist())
            test_indices.extend(sub.index[n_ref:].tolist())
            print(f"[INFO] [classify] pop={pop}: total={n}, ref={n_ref}, test={n - n_ref}")

        ref_df = df.loc[ref_indices, [sample_col, pop_col]].reset_index(drop=True)
        test_df = df.loc[test_indices, [sample_col, pop_col]].reset_index(drop=True)

    else:
        # === mode 2: admix mode —— automatically identify admix populations ===
        pops = sorted(df[pop_col].unique())
        admix_pops = [p for p in pops if "admix" in str(p).lower()]
        source_pops = [p for p in pops if p not in admix_pops]

        if len(admix_pops) == 0:
            raise ValueError(
                "In 'admix' mode, no population name contains 'admix'.\n"
                "Please check your meta table or rename admixed populations to include 'admix'."
            )

        print(f"[INFO] [admix] detected admix pops   = {admix_pops}")
        print(f"[INFO] [admix] detected source pops  = {source_pops}")

        ref_df = df[df[pop_col].isin(source_pops)][[sample_col, pop_col]].reset_index(drop=True)
        test_df = df[df[pop_col].isin(admix_pops)][[sample_col, pop_col]].reset_index(drop=True)

        print(f"[INFO] [admix] ref={len(ref_df)}, test={len(test_df)}")

    # 3. write (note add sep)
    ref_df.to_csv(ref_out, index=False, sep=",")
    test_df.to_csv(test_out, index=False, sep=",")

    print(f"[OK] reference list -> {ref_out}")
    print(f"[OK] test list      -> {test_out}")

    return ref_df, test_df
